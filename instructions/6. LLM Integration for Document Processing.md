
# Plan 06 â€” LLM Integration for Document Processing

## Goal
Use LLMs to enhance document understanding and searchability:
- Generate document-level summaries from OCR text
- Extract and structure metadata for indexing
- Improve search quality with semantic understanding
- Keep prompts generic to handle diverse document types

**Note:** Folder descriptions are user-written. LLM focuses on individual documents.

---

## Core Principles
- LLM calls are expensive and async (run via job queue)
- Prompts are generic to handle diverse document types
- Results stored in DB, regeneration only when needed
- Graceful fallback if LLM unavailable
- Iterative approach - prompts will be refined based on real usage

---

## Technology Stack
- LLM Provider: **OpenAI API** (GPT-4o or GPT-4o-mini)
- Alternative: Local models via Ollama (future)
- API Client: OpenAI Node.js SDK
- Job Queue: BullMQ (from Plan 04)

---

## LLM Workflow

### 1. Trigger
Document LLM processing triggered by:
- OCR completes successfully
- User explicitly requests re-summarization
- Manual trigger via API

### 2. Context Gathering
Collect data for LLM prompt:
- OCR raw text (truncated if extremely long)
- OCR metadata (dates, values already extracted by OCR)
- Document filename (provides context)
- Document category (if already classified)

### 3. Prompt Construction
Build structured prompt with:
- System message (generic document analyzer role)
- Context (OCR text + metadata)
- Instructions (generate summary, enhance metadata)
- Output format (structured JSON)

### 4. LLM Call
- Send prompt to OpenAI API
- Set max tokens (300-500 for document summary)
- Parse response

### 5. Storage
- Save summary to `documents.llm_summary` field
- Store enhanced metadata in `documents.llm_metadata` (JSONB)
- Update search indexes with enriched data

---

## Folder Structure

```
backend/
  src/
    llm/
      llm.service.ts           // Main LLM orchestration
      openai.client.ts         // OpenAI API wrapper
      prompt-builder.ts        // Construct prompts
      document-processor.ts    // Document-level LLM logic
      metadata-enhancer.ts     // Enhance OCR metadata with LLM

    workers/
      llm.worker.ts            // Job queue worker (from Plan 04)
```

---

## Prompt Design

### System Message (Generic)
```
You are a document analysis assistant. Your task is to analyze text extracted from a document via OCR and provide a structured summary.

Your goals:
- Generate a concise, human-readable summary (2-3 sentences)
- Extract key entities (people, organizations, places)
- Identify main topics or themes
- Suggest a document type/category
- Extract important information that would be useful for searching

Be factual and concise. Avoid speculation.
```

### Context Format
```
Document: "IMG_2023_05_15.jpg"
OCR Text:
---
[Full OCR text here, up to ~2000 chars]
---

OCR Metadata (already extracted):
- Detected dates: 2023-05-15
- Extracted values: $1,234.56, 150 shares
- OCR confidence: 87%
```

### Instructions
```
Based on this document, generate:
1. A 2-3 sentence summary of what this document is about
2. A suggested title for the document (5-10 words)
3. Key entities mentioned (people, organizations, places)
4. Main topics or themes
5. A document type/category
6. Any additional important information for indexing

Respond in JSON format:
{
  "summary": "Brief description of the document contents...",
  "title": "Suggested Document Title",
  "key_entities": ["Entity 1", "Entity 2"],
  "topics": ["topic1", "topic2"],
  "document_type": "category_name",
  "key_values": [
    { "label": "context for value", "value": "extracted value" }
  ],
  "sentiment": "neutral",
  "additional_dates": ["2023-06-01"]
}
```

### Example Prompt/Response

**Input:**
```
OCR Text: "Apple Inc. Quarterly Statement - Q1 2023. Shares held: 150. Current value: $24,750. Dividend received: $67.50. Date: January 15, 2023"
```

**Output:**
```json
{
  "summary": "Quarterly investment statement for Apple Inc. stock holdings in Q1 2023, showing 150 shares valued at $24,750 with a dividend payment of $67.50.",
  "title": "Apple Inc Q1 2023 Statement",
  "key_entities": ["Apple Inc."],
  "topics": ["investments", "stocks", "dividends"],
  "document_type": "financial_statement",
  "key_values": [
    { "label": "Shares held", "value": "150" },
    { "label": "Portfolio value", "value": "$24,750" },
    { "label": "Dividend", "value": "$67.50" }
  ],
  "sentiment": "neutral",
  "additional_dates": []
}
```

---

## LLM Service Interface

```typescript
interface LlmService {
  processDocument(documentId: string): Promise<DocumentLlmResult>
  enhanceMetadata(ocrText: string, ocrMetadata: any): Promise<EnhancedMetadata>
  buildPrompt(documentId: string): Promise<LlmPrompt>
}

interface DocumentLlmResult {
  summary: string                 // Human-readable summary (2-3 sentences)
  enhanced_metadata: EnhancedMetadata
  token_count: number            // For cost tracking
}

interface EnhancedMetadata {
  title?: string                 // Suggested document title
  key_entities: string[]         // People, companies, places mentioned
  topics: string[]               // Main topics/themes
  sentiment?: 'positive' | 'neutral' | 'negative'
  document_type?: string         // Better classification than OCR alone
  extracted_dates?: string[]     // Additional dates LLM found
  key_values?: { label: string, value: string }[]  // Important numbers with context
}

interface LlmPrompt {
  system: string
  user: string
  max_tokens: number
}
```

---

## Cost Optimization

### Token Management
- Truncate very long OCR text (max ~2000 chars for input)
- Skip LLM processing if OCR confidence is extremely low (<30%)
- Most documents will be 500-1500 tokens total

### Caching
- Store LLM results in `documents.llm_summary` and `documents.llm_metadata`
- Don't regenerate unless user explicitly requests
- If OCR is re-run, optionally trigger LLM re-processing

### Model Selection
- Use `gpt-4o-mini` by default (cheap, fast, good enough)
- Use `gpt-4o` only if user requests higher quality analysis
- Configurable per-document or via environment variable

---

## Error Handling

### API Failures
- Network timeout: Retry with backoff
- Rate limit: Queue with delay
- Invalid API key: Fail immediately, log error

### Invalid Responses
- Malformed JSON: Parse what's possible, use plain text as fallback
- Empty summary: Generate basic summary from metadata without LLM

### Fallback Strategy
If LLM unavailable, generate simple summary:
```
"This folder contains 15 documents from 01/15/2023 to 12/31/2023. 
Categories: stock_overview, dividend_statement. 
Companies: Apple Inc., Microsoft Corp., Johnson & Johnson."
```

---

## Integration with Job Queue

LLM Worker flow:
```typescript
async function processLlmJob(job: Job<LlmJobData>) {
  const { documentId } = job.data
  
  // 1. Fetch document and OCR result
  const document = await db
    .selectFrom('documents')
    .where('id', '=', documentId)
    .selectAll()
    .executeTakeFirst()
  
  const ocrResult = await db
    .selectFrom('ocr_results')
    .where('document_id', '=', documentId)
    .selectAll()
    .executeTakeFirst()
  
  if (!ocrResult || !ocrResult.raw_text) {
    throw new Error('No OCR result found for document')
  }
  
  // 2. Build prompt
  const prompt = buildDocumentPrompt(document, ocrResult)
  
  // 3. Call OpenAI
  const response = await openaiClient.chat.completions.create({
    model: process.env.OPENAI_MODEL || 'gpt-4o-mini',
    messages: [
      { role: 'system', content: prompt.system },
      { role: 'user', content: prompt.user }
    ],
    max_tokens: 500,
    temperature: 0.3,  // Lower for more consistent output
    response_format: { type: 'json_object' }  // Ensure JSON response
  })
  
  // 4. Parse response
  const result = JSON.parse(response.choices[0].message.content)
  
  // 5. Save to DB
  await db.updateTable('documents')
    .set({
      llm_summary: result.summary,
      llm_metadata: JSON.stringify(result),
      llm_processed_at: new Date(),
      llm_token_count: response.usage.total_tokens
    })
    .where('id', '=', documentId)
    .execute()
  
  // 6. Optionally update search index with enhanced metadata
  await updateSearchIndex(documentId, result)
  
  return { 
    success: true, 
    token_count: response.usage.total_tokens,
    summary: result.summary
  }
}
```

---

## API Endpoints

### Trigger LLM Processing
```
POST /api/documents/:id/process-llm
Response: { job_id: string, status: 'pending' }
```

### Get Document with LLM Data
```
GET /api/documents/:id
Response: {
  id: string,
  filename: string,
  llm_summary: string,
  llm_metadata: {
    title: string,
    key_entities: string[],
    topics: string[],
    document_type: string,
    ...
  },
  llm_processed_at: string,
  ocr_text: string,
  ...
}
```

### Batch Process Multiple Documents
```
POST /api/documents/batch-process-llm
Body: { document_ids: string[] }
Response: { job_ids: string[] }
```

### Reprocess Document
```
POST /api/documents/:id/reprocess-llm
Response: { job_id: string }
```

---

## Configuration

Environment variables:
```
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini  # or gpt-4o
OPENAI_MAX_TOKENS=1000
OPENAI_TEMPERATURE=0.3
LLM_ENABLED=true  # Feature flag
```

---

## Monitoring

### Cost Tracking
- Log token usage per request
- Store cumulative token count per month
- Alert if exceeding budget threshold

### Quality Monitoring
- Log generated summaries for review
- Track user feedback (useful/not useful)
- A/B test different prompt variants

---

## Acceptance Criteria
- OpenAI API client configured
- LLM worker processes document jobs successfully
- Document summaries are generated for test data
- Results are stored in `documents.llm_summary` and `documents.llm_metadata`
- Token usage is tracked per document
- Fallback works if API unavailable or errors occur
- API endpoints return LLM data correctly
- Generic prompts work across different document types (not just financial)

---

## Iterative Development Approach

**This plan will evolve based on real usage.** Key areas for iteration:

### Prompt Refinement
- Test with diverse document types (receipts, letters, forms, contracts, etc.)
- Adjust prompts based on quality of results
- A/B test different prompt variations
- Gather user feedback on summary quality

### Metadata Extraction
- Add/remove fields based on what's actually useful
- Fine-tune entity extraction accuracy
- Improve document type classification

### Performance vs Cost
- Monitor token usage and costs
- Optimize prompt length vs quality tradeoff
- Consider batch processing strategies

---

## Future Enhancements
- Support for local LLMs (Ollama, llama.cpp) to reduce costs
- Multi-language support (detect document language, adjust prompts)
- Custom prompt templates per document type
- User feedback loop (rate summaries, correct metadata)
- Vector embeddings for semantic search
- "Chat with document" feature (Q&A about specific documents)
- Confidence scores for LLM outputs
- Document comparison ("How is this different from that document?")


