
# Plan 06 — LLM Integration for Document Processing

## Goal
Use LLMs to enhance document understanding and searchability:
- Generate document-level summaries from OCR text
- Extract and structure metadata for indexing
- Improve search quality with semantic understanding
- Keep prompts generic to handle diverse document types

**Note:** Folder descriptions are user-written. LLM focuses on individual documents.

---

## Core Principles
- LLM calls are expensive and async (run via job queue)
- Prompts are generic to handle diverse document types
- Results stored in DB, regeneration only when needed
- Graceful fallback if LLM unavailable
- Iterative approach - prompts will be refined based on real usage

---

## Technology Stack
- LLM Provider: **OpenAI API** (GPT-4o or GPT-4o-mini)
- Alternative: Local models via Ollama (future)
- API Client: OpenAI Node.js SDK
- Job Queue: BullMQ (from Plan 04)

---

## Processing Eligibility Rules

Not all uploaded files should be processed by LLM. This section defines what gets processed and what gets skipped.

### Supported File Types

| File Type | Processing Path | Notes |
|-----------|----------------|-------|
| **Images** (jpg, png, webp, etc.) | OCR → LLM text summary OR vision | Handled by Plan 05 first |
| **PDF** | Text extraction → LLM summary | Future: OCR for scanned PDFs |
| **Plain text** (.txt, .md, .csv) | Direct text → LLM summary | Size limits apply |
| **Office docs** (.docx, .xlsx, .pptx) | Text extraction → LLM summary | Future enhancement |
| **Code files** (.js, .py, .ts, etc.) | Skip or minimal processing | Not useful to summarize |
| **Binary/media** (video, audio, zip) | Skip entirely | No text to extract |

### Size Limits

LLM processing is expensive. Large files must be handled with smart sampling:

| Metric | Limit | Action |
|--------|-------|--------|
| **Max input text** | 50,000 chars (~12,500 tokens) | Truncate with note |
| **Large text** | 50K - 500K chars | Sample from beginning, middle, end |
| **Very large text** | >500K chars | Sample snippets from throughout |
| **Image file size** | 20 MB | Handled by OCR limits |

**Key principle:** Never skip a file entirely just because it's large. The LLM can infer the essence of a document from representative snippets.

### Text Sampling Strategy

For files that exceed the token limit, sample strategically:

```typescript
interface PreparedText {
  text: string;
  truncated: boolean;
  samplingStrategy: 'full' | 'start_end' | 'distributed';
  originalLength: number;
  sampledSections: number;
}

function prepareTextForLlm(rawText: string): PreparedText {
  const MAX_CHARS = 50_000;
  const LARGE_THRESHOLD = 50_000;
  const VERY_LARGE_THRESHOLD = 500_000;
  
  // Small files: use full text
  if (rawText.length <= MAX_CHARS) {
    return { 
      text: rawText, 
      truncated: false, 
      samplingStrategy: 'full',
      originalLength: rawText.length,
      sampledSections: 1,
    };
  }
  
  // Medium files (50K - 500K): beginning + middle + end
  if (rawText.length <= VERY_LARGE_THRESHOLD) {
    return sampleStartMiddleEnd(rawText, MAX_CHARS);
  }
  
  // Very large files (>500K): distributed sampling
  return sampleDistributed(rawText, MAX_CHARS);
}

function sampleStartMiddleEnd(text: string, maxChars: number): PreparedText {
  const startSize = Math.floor(maxChars * 0.50);   // 50% from beginning
  const middleSize = Math.floor(maxChars * 0.25);  // 25% from middle
  const endSize = Math.floor(maxChars * 0.20);     // 20% from end
  
  const middleStart = Math.floor(text.length / 2) - Math.floor(middleSize / 2);
  
  const sampledText = [
    text.slice(0, startSize),
    `\n\n[... ${((middleStart - startSize) / 1000).toFixed(0)}K chars omitted ...]\n\n`,
    text.slice(middleStart, middleStart + middleSize),
    `\n\n[... ${((text.length - middleStart - middleSize - endSize) / 1000).toFixed(0)}K chars omitted ...]\n\n`,
    text.slice(-endSize),
  ].join('');
  
  return {
    text: sampledText,
    truncated: true,
    samplingStrategy: 'start_end',
    originalLength: text.length,
    sampledSections: 3,
  };
}

function sampleDistributed(text: string, maxChars: number): PreparedText {
  // For very large files, take evenly distributed snippets
  const snippetSize = 5_000;  // 5K chars per snippet
  const numSnippets = Math.floor(maxChars / snippetSize);  // ~10 snippets
  const spacing = Math.floor(text.length / numSnippets);
  
  const snippets: string[] = [];
  for (let i = 0; i < numSnippets; i++) {
    const start = i * spacing;
    const snippet = text.slice(start, start + snippetSize);
    const position = ((start / text.length) * 100).toFixed(0);
    snippets.push(`[Section at ${position}% of document]\n${snippet}`);
  }
  
  const sampledText = [
    `[Document is ${(text.length / 1000).toFixed(0)}K chars. Showing ${numSnippets} distributed samples:]\n\n`,
    snippets.join('\n\n[...]\n\n'),
  ].join('');
  
  return {
    text: sampledText,
    truncated: true,
    samplingStrategy: 'distributed',
    originalLength: text.length,
    sampledSections: numSnippets,
  };
}
```

### Prompt Adjustment for Sampled Text

When text is sampled, adjust the prompt to inform the LLM:

```typescript
function buildPromptWithSamplingContext(prepared: PreparedText): string {
  if (!prepared.truncated) {
    return prepared.text;
  }
  
  const contextNote = prepared.samplingStrategy === 'distributed'
    ? `Note: This is a very large document (${(prepared.originalLength / 1000).toFixed(0)}K chars). ` +
      `You are seeing ${prepared.sampledSections} representative samples from throughout the document. ` +
      `Infer the overall theme and purpose from these samples.`
    : `Note: This document has been truncated from ${(prepared.originalLength / 1000).toFixed(0)}K chars. ` +
      `You are seeing the beginning, middle, and end sections.`;
  
  return `${contextNote}\n\n---\n\n${prepared.text}`;
}
```
```

### Eligibility Check Function

```typescript
interface LlmEligibility {
  eligible: boolean;
  reason?: string;
  processingType?: 'text_summary' | 'vision_describe' | 'skip';
  warnings?: string[];
}

function checkLlmEligibility(document: Document, ocrResult?: OcrResult): LlmEligibility {
  const { mime_type, size_bytes, has_meaningful_text } = document;
  
  // 1. Check if LLM is enabled globally
  if (!env.LLM_ENABLED) {
    return { eligible: false, reason: 'llm_disabled', processingType: 'skip' };
  }
  
  // 2. Check file type
  const category = getFileCategory(mime_type);
  
  if (category === 'binary' || category === 'media') {
    return { eligible: false, reason: 'unsupported_file_type', processingType: 'skip' };
  }
  
  if (category === 'code') {
    // Code files: optional, usually skip
    if (!env.LLM_PROCESS_CODE_FILES) {
      return { eligible: false, reason: 'code_file_skipped', processingType: 'skip' };
    }
  }
  
  // 3. Check for images without text
  if (category === 'image' && !has_meaningful_text) {
    if (env.LLM_VISION_ENABLED) {
      return { eligible: true, processingType: 'vision_describe' };
    }
    return { eligible: false, reason: 'no_text_no_vision', processingType: 'skip' };
  }
  
  // 4. Check text size
  const textLength = ocrResult?.raw_text?.length || 0;
  
  if (textLength === 0) {
    return { eligible: false, reason: 'no_text_content', processingType: 'skip' };
  }
  
  // Note: We never skip due to size - we sample instead
  // Large files will be handled by prepareTextForLlm() with distributed sampling
  
  // 5. Check confidence (for OCR'd content)
  if (ocrResult && ocrResult.confidence_score !== null && ocrResult.confidence_score < 30) {
    return { 
      eligible: false, 
      reason: 'ocr_confidence_too_low', 
      processingType: 'skip',
      warnings: ['OCR confidence below 30%, skipping LLM to avoid garbage-in-garbage-out']
    };
  }
  
  // Eligible for text summary
  const warnings: string[] = [];
  if (textLength > 50_000) {
    warnings.push(`Text will be truncated from ${textLength} to ~50,000 chars`);
  }
  
  return { 
    eligible: true, 
    processingType: 'text_summary',
    warnings: warnings.length > 0 ? warnings : undefined
  };
}

function getFileCategory(mimeType: string): 'image' | 'document' | 'text' | 'code' | 'media' | 'binary' {
  if (mimeType.startsWith('image/')) return 'image';
  if (mimeType.startsWith('video/') || mimeType.startsWith('audio/')) return 'media';
  if (mimeType === 'application/pdf') return 'document';
  if (mimeType.startsWith('application/vnd.openxmlformats')) return 'document'; // Office docs
  if (mimeType === 'text/plain' || mimeType === 'text/markdown' || mimeType === 'text/csv') return 'text';
  if (mimeType.includes('javascript') || mimeType.includes('typescript') || 
      mimeType.includes('python') || mimeType.includes('java') ||
      mimeType === 'text/x-python' || mimeType === 'application/json') return 'code';
  if (mimeType.startsWith('application/')) return 'binary';
  return 'text'; // Default to text for unknown
}
```

### Skip Reasons (stored in DB)

When LLM processing is skipped, store the reason:

```typescript
type LlmSkipReason = 
  | 'llm_disabled'           // LLM feature flag is off
  | 'unsupported_file_type'  // Binary, media, etc.
  | 'code_file_skipped'      // Code files not processed
  | 'no_text_no_vision'      // Image without text, vision disabled
  | 'no_text_content'        // Empty or no extractable text
  | 'ocr_confidence_too_low' // OCR quality too poor
  | 'user_opted_out';        // User disabled for this document

// Note: We never skip due to text size - large files are sampled instead
```

Store in `documents.llm_metadata`:
```json
{
  "skipped": true,
  "skip_reason": "no_text_content",
  "skipped_at": "2024-01-15T10:30:00Z"
}
```

For sampled (large) files, store sampling info:
```json
{
  "type": "text_summary",
  "truncated": true,
  "sampling_strategy": "distributed",
  "original_text_length": 1500000,
  "sampled_sections": 10,
  "summary": "..."
}
```

### Configuration

```
# Processing limits
LLM_MAX_INPUT_CHARS=50000           # Max chars to send to LLM (rest is sampled)
LLM_SNIPPET_SIZE=5000               # Size of each snippet for distributed sampling
LLM_PROCESS_CODE_FILES=false        # Whether to summarize code files
LLM_MIN_OCR_CONFIDENCE=30           # Minimum OCR confidence to process
```

---

## LLM Workflow

### 0. Pre-check: Text vs Image

Before processing, check `documents.has_meaningful_text` (set by OCR in Plan 05):

- **If `has_meaningful_text = true`**: Proceed with text-based summarization (default flow)
- **If `has_meaningful_text = false`**: 
  - Skip text summarization entirely (no text to summarize)
  - Optionally use **vision model** to describe image content (if enabled)
  - See "Vision Model for Photos" section below

### 1. Trigger
Document LLM processing triggered by:
- OCR completes successfully **AND** `has_meaningful_text = true`
- User explicitly requests re-summarization
- Manual trigger via API
- For photos: OCR completes AND `LLM_VISION_ENABLED = true`

### 2. Context Gathering
Collect data for LLM prompt:
- OCR raw text (truncated if extremely long)
- OCR metadata (dates, values already extracted by OCR)
- Document filename (provides context)
- Document category (if already classified)
- `has_meaningful_text` flag (determines processing path)

### 3. Prompt Construction
Build structured prompt with:
- System message (generic document analyzer role)
- Context (OCR text + metadata)
- Instructions (generate summary, enhance metadata)
- Output format (structured JSON)

### 4. LLM Call
- Send prompt to OpenAI API
- Set max tokens (300-500 for document summary)
- Parse response

### 5. Storage
- Save summary to `documents.llm_summary` field
- Store enhanced metadata in `documents.llm_metadata` (JSONB)
- Update search indexes with enriched data

---

## Folder Structure

```
backend/
  src/
    llm/
      llm.service.ts           // Main LLM orchestration
      openai.client.ts         // OpenAI API wrapper
      prompt-builder.ts        // Construct prompts
      document-processor.ts    // Document-level LLM logic
      metadata-enhancer.ts     // Enhance OCR metadata with LLM

    workers/
      llm.worker.ts            // Job queue worker (from Plan 04)
```

---

## Prompt Design

### System Message (Generic)
```
You are a document analysis assistant. Your task is to analyze text extracted from a document via OCR and provide a structured summary.

Your goals:
- Generate a concise, human-readable summary (2-3 sentences)
- Extract key entities (people, organizations, places)
- Identify main topics or themes
- Suggest a document type/category
- Extract important information that would be useful for searching

Be factual and concise. Avoid speculation.
```

### Context Format
```
Document: "IMG_2023_05_15.jpg"
OCR Text:
---
[Full OCR text here, up to ~2000 chars]
---

OCR Metadata (already extracted):
- Detected dates: 2023-05-15
- Extracted values: $1,234.56, 150 shares
- OCR confidence: 87%
```

### Instructions
```
Based on this document, generate:
1. A 2-3 sentence summary of what this document is about
2. A suggested title for the document (5-10 words)
3. Key entities mentioned (people, organizations, places)
4. Main topics or themes
5. A document type/category
6. Any additional important information for indexing

Respond in JSON format:
{
  "summary": "Brief description of the document contents...",
  "title": "Suggested Document Title",
  "key_entities": ["Entity 1", "Entity 2"],
  "topics": ["topic1", "topic2"],
  "document_type": "category_name",
  "key_values": [
    { "label": "context for value", "value": "extracted value" }
  ],
  "sentiment": "neutral",
  "additional_dates": ["2023-06-01"]
}
```

### Example Prompt/Response

**Input:**
```
OCR Text: "Apple Inc. Quarterly Statement - Q1 2023. Shares held: 150. Current value: $24,750. Dividend received: $67.50. Date: January 15, 2023"
```

**Output:**
```json
{
  "summary": "Quarterly investment statement for Apple Inc. stock holdings in Q1 2023, showing 150 shares valued at $24,750 with a dividend payment of $67.50.",
  "title": "Apple Inc Q1 2023 Statement",
  "key_entities": ["Apple Inc."],
  "topics": ["investments", "stocks", "dividends"],
  "document_type": "financial_statement",
  "key_values": [
    { "label": "Shares held", "value": "150" },
    { "label": "Portfolio value", "value": "$24,750" },
    { "label": "Dividend", "value": "$67.50" }
  ],
  "sentiment": "neutral",
  "additional_dates": []
}
```

---

## LLM Service Interface

```typescript
interface LlmService {
  processDocument(documentId: string): Promise<DocumentLlmResult>
  enhanceMetadata(ocrText: string, ocrMetadata: any): Promise<EnhancedMetadata>
  buildPrompt(documentId: string): Promise<LlmPrompt>
}

interface DocumentLlmResult {
  summary: string                 // Human-readable summary (2-3 sentences)
  enhanced_metadata: EnhancedMetadata
  token_count: number            // For cost tracking
}

interface EnhancedMetadata {
  title?: string                 // Suggested document title
  key_entities: string[]         // People, companies, places mentioned
  topics: string[]               // Main topics/themes
  sentiment?: 'positive' | 'neutral' | 'negative'
  document_type?: string         // Better classification than OCR alone
  extracted_dates?: string[]     // Additional dates LLM found
  key_values?: { label: string, value: string }[]  // Important numbers with context
}

interface LlmPrompt {
  system: string
  user: string
  max_tokens: number
}
```

---

## Cost Optimization

### Token Management
- Truncate very long OCR text (max ~2000 chars for input)
- Skip LLM processing if OCR confidence is extremely low (<30%)
- **Skip text summarization entirely if `has_meaningful_text = false`** (no text to summarize)
- Most documents will be 500-1500 tokens total

### Caching
- Store LLM results in `documents.llm_summary` and `documents.llm_metadata`
- Don't regenerate unless user explicitly requests
- If OCR is re-run, optionally trigger LLM re-processing

### Model Selection
- Use `gpt-4o-mini` by default (cheap, fast, good enough)
- Use `gpt-4o` only if user requests higher quality analysis
- Configurable per-document or via environment variable

---

## Vision Model for Photos (Optional)

For images without meaningful text (`has_meaningful_text = false`), optionally use GPT-4o's vision capability to generate descriptions.

### When to Use
- `LLM_VISION_ENABLED = true` in environment
- Document category is `photo`, `screenshot`, or `graphic`
- User hasn't disabled vision processing

### Vision Prompt
```
Describe this image in 2-3 sentences. Include:
- What the image shows (people, places, objects, scenes)
- Any notable details or context
- If it's a screenshot, describe the app/content shown

Be concise and factual.
```

### Vision Response Storage
```typescript
interface VisionResult {
  description: string;          // "A beach sunset with two people walking along the shore"
  detected_objects?: string[];  // ["beach", "sunset", "people", "ocean"]
  scene_type?: string;          // "outdoor", "indoor", "screenshot", "artwork"
  has_people?: boolean;
  has_text?: boolean;           // True if vision model detects text we missed
}
```

Store in `documents.llm_metadata` with `type: 'vision'` to distinguish from text summaries.

### Cost Considerations
- Vision API is more expensive than text (~$0.01-0.02 per image for GPT-4o)
- Consider making this opt-in or batch-only
- Can use cheaper models for basic scene detection

### Configuration
```
LLM_VISION_ENABLED=true        # Enable vision processing for photos
LLM_VISION_MODEL=gpt-4o        # Vision-capable model
LLM_VISION_MAX_IMAGES=100      # Monthly limit for vision API calls
```

---

## Error Handling

### API Failures
- Network timeout: Retry with backoff
- Rate limit: Queue with delay
- Invalid API key: Fail immediately, log error

### Invalid Responses
- Malformed JSON: Parse what's possible, use plain text as fallback
- Empty summary: Generate basic summary from metadata without LLM

### Fallback Strategy
If LLM unavailable, generate simple summary:
```
"This folder contains 15 documents from 01/15/2023 to 12/31/2023. 
Categories: stock_overview, dividend_statement. 
Companies: Apple Inc., Microsoft Corp., Johnson & Johnson."
```

---

## Integration with Job Queue

LLM Worker flow:
```typescript
interface LlmJobData {
  documentId: string
  type?: 'text_summary' | 'vision_describe'  // Optional - will be determined by eligibility check if not provided
}

async function processLlmJob(job: Job<LlmJobData>) {
  const { documentId, type } = job.data
  
  // 1. Fetch document and OCR result
  const document = await db
    .selectFrom('documents')
    .where('id', '=', documentId)
    .selectAll()
    .executeTakeFirst()
  
  if (!document) {
    throw new Error('Document not found')
  }
  
  const ocrResult = await db
    .selectFrom('ocr_results')
    .where('document_id', '=', documentId)
    .selectAll()
    .executeTakeFirst()
  
  // 2. Check eligibility (unless type is explicitly provided)
  const eligibility = checkLlmEligibility(document, ocrResult)
  const processingType = type || eligibility.processingType
  
  if (!eligibility.eligible && !type) {
    // Store skip reason in metadata
    await db.updateTable('documents')
      .set({
        llm_metadata: JSON.stringify({
          skipped: true,
          skip_reason: eligibility.reason,
          original_text_length: ocrResult?.raw_text?.length,
          skipped_at: new Date().toISOString(),
          warnings: eligibility.warnings,
        }),
        llm_processed_at: new Date(),
      })
      .where('id', '=', documentId)
      .execute()
    
    logger.info('LLM processing skipped', { 
      documentId, 
      reason: eligibility.reason,
      warnings: eligibility.warnings 
    })
    
    return { success: true, skipped: true, reason: eligibility.reason }
  }
  
  // 3. Log warnings if any
  if (eligibility.warnings) {
    logger.warn('LLM processing warnings', { documentId, warnings: eligibility.warnings })
  }
  
  // 4. Route to appropriate processor
  if (processingType === 'vision_describe') {
    return processVisionJob(document)
  }
  
  // Text summary flow (default)
  return processTextSummaryJob(document, ocrResult!)
}

async function processTextSummaryJob(document: Document, ocrResult: OcrResult) {
  // Prepare text with sampling if needed
  const prepared = prepareTextForLlm(ocrResult.raw_text)
  
  // Build prompt with sampling context if truncated
  const textForPrompt = buildPromptWithSamplingContext(prepared)
  
  // Build full prompt
  const prompt = buildDocumentPrompt(document, { 
    ...ocrResult, 
    raw_text: textForPrompt,
    was_truncated: prepared.truncated,
    sampling_strategy: prepared.samplingStrategy,
    original_length: prepared.originalLength,
  })
  
  // Call OpenAI with prepared text
  const response = await openaiClient.chat.completions.create({
    model: process.env.OPENAI_MODEL || 'gpt-4o-mini',
    messages: [
      { role: 'system', content: prompt.system },
      { role: 'user', content: prompt.user }
    ],
    max_tokens: 500,
    temperature: 0.3,
    response_format: { type: 'json_object' }
  })
  
  const result = JSON.parse(response.choices[0].message.content)
  
  await db.updateTable('documents')
    .set({
      llm_summary: result.summary,
      llm_metadata: JSON.stringify({ 
        ...result, 
        type: 'text_summary',
        truncated: prepared.truncated,
        sampling_strategy: prepared.samplingStrategy,
        original_text_length: prepared.originalLength,
        sampled_sections: prepared.sampledSections,
      }),
      llm_processed_at: new Date(),
      llm_token_count: response.usage.total_tokens
    })
    .where('id', '=', document.id)
    .execute()
  
  await updateSearchIndex(document.id, result)
  
  return { 
    success: true, 
    token_count: response.usage.total_tokens,
    summary: result.summary,
    truncated: prepared.truncated,
    sampling_strategy: prepared.samplingStrategy,
    original_text_length: prepared.originalLength,
  }
}

async function processVisionJob(document: Document) {
  if (!env.LLM_VISION_ENABLED) {
    return { success: true, skipped: true, reason: 'vision_disabled' }
  }
  
  // Load image from storage
  const imageBuffer = await storageService.retrieve(document.file_path)
  const base64Image = imageBuffer.toString('base64')
  const mimeType = document.mime_type
  
  const response = await openaiClient.chat.completions.create({
    model: env.LLM_VISION_MODEL || 'gpt-4o',
    messages: [
      {
        role: 'user',
        content: [
          {
            type: 'text',
            text: `Describe this image in 2-3 sentences. Include what the image shows (people, places, objects, scenes) and any notable details. If it's a screenshot, describe the app/content shown. Be concise and factual.

Respond in JSON format:
{
  "description": "Brief description of the image",
  "detected_objects": ["object1", "object2"],
  "scene_type": "outdoor|indoor|screenshot|artwork|other",
  "has_people": true/false
}`
          },
          {
            type: 'image_url',
            image_url: {
              url: `data:${mimeType};base64,${base64Image}`
            }
          }
        ]
      }
    ],
    max_tokens: 300,
    response_format: { type: 'json_object' }
  })
  
  const result = JSON.parse(response.choices[0].message.content)
  
  await db.updateTable('documents')
    .set({
      llm_summary: result.description,
      llm_metadata: JSON.stringify({ ...result, type: 'vision_describe' }),
      llm_processed_at: new Date(),
      llm_token_count: response.usage.total_tokens
    })
    .where('id', '=', document.id)
    .execute()
  
  // Optionally index the description for search
  if (result.description) {
    await updateSearchIndex(document.id, { summary: result.description })
  }
  
  return { 
    success: true, 
    token_count: response.usage.total_tokens,
    description: result.description
  }
}
```

---

## API Endpoints

### Trigger LLM Processing
```
POST /api/documents/:id/process-llm
Response: { job_id: string, status: 'pending' }
```

### Get Document with LLM Data
```
GET /api/documents/:id
Response: {
  id: string,
  filename: string,
  llm_summary: string,
  llm_metadata: {
    title: string,
    key_entities: string[],
    topics: string[],
    document_type: string,
    ...
  },
  llm_processed_at: string,
  ocr_text: string,
  ...
}
```

### Batch Process Multiple Documents
```
POST /api/documents/batch-process-llm
Body: { document_ids: string[] }
Response: { job_ids: string[] }
```

### Reprocess Document
```
POST /api/documents/:id/reprocess-llm
Response: { job_id: string }
```

---

## Configuration

Environment variables:
```
# Text summarization
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini  # or gpt-4o
OPENAI_MAX_TOKENS=1000
OPENAI_TEMPERATURE=0.3
LLM_ENABLED=true  # Feature flag

# Vision processing for photos (optional)
LLM_VISION_ENABLED=false       # Enable vision processing for photos/graphics
LLM_VISION_MODEL=gpt-4o        # Vision-capable model (must support images)
LLM_VISION_MAX_IMAGES=100      # Monthly limit for vision API calls (cost control)
```

---

## Monitoring

### Cost Tracking
- Log token usage per request
- Store cumulative token count per month
- Alert if exceeding budget threshold

### Quality Monitoring
- Log generated summaries for review
- Track user feedback (useful/not useful)
- A/B test different prompt variants

---

## Acceptance Criteria
- OpenAI API client configured
- LLM worker processes document jobs successfully
- Document summaries are generated for test data
- Results are stored in `documents.llm_summary` and `documents.llm_metadata`
- Token usage is tracked per document
- Fallback works if API unavailable or errors occur
- API endpoints return LLM data correctly
- Generic prompts work across different document types (not just financial)
- **Documents with `has_meaningful_text = false` skip text summarization**
- **Vision processing works for photos when `LLM_VISION_ENABLED = true`**
- **Job type (`text_summary` vs `vision_describe`) is correctly routed**
- **Large text files use smart sampling (start/middle/end or distributed)**
- **Sampling strategy is correctly chosen based on file size**
- **LLM prompt includes context about sampling when text is truncated**
- **Binary/media files are not queued for LLM processing**
- **Skip reasons are properly logged and stored**

---

## Iterative Development Approach

**This plan will evolve based on real usage.** Key areas for iteration:

### Prompt Refinement
- Test with diverse document types (receipts, letters, forms, contracts, etc.)
- Adjust prompts based on quality of results
- A/B test different prompt variations
- Gather user feedback on summary quality

### Metadata Extraction
- Add/remove fields based on what's actually useful
- Fine-tune entity extraction accuracy
- Improve document type classification

### Performance vs Cost
- Monitor token usage and costs
- Optimize prompt length vs quality tradeoff
- Consider batch processing strategies

---

## Future Enhancements
- Support for local LLMs (Ollama, llama.cpp) to reduce costs
- Multi-language support (detect document language, adjust prompts)
- Custom prompt templates per document type
- User feedback loop (rate summaries, correct metadata)
- Vector embeddings for semantic search
- "Chat with document" feature (Q&A about specific documents)
- Confidence scores for LLM outputs
- Document comparison ("How is this different from that document?")


