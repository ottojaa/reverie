# Plan 05 — OCR Pipeline

## Goal

Implement automated OCR processing for uploaded documents:

- Extract text from documents (PDF, images, etc.)
- Parse structured metadata (dates, values, categories)
- Store results for search and LLM processing
- Handle batch uploads efficiently (100+ documents at a time)
- Upload progress is shown to user. After upload is complete, the OCR process progress is shown in the UI and user can navigate away from the page.
- **Detect and handle images without meaningful text** (photos, graphics, etc.)

---

## Core Principles

- OCR runs asynchronously via job queue
- Results are structured and searchable
- Confidence scores tracked for quality control
- Handles multiple image formats
- Fails gracefully on poor-quality images
- **Distinguishes documents from photos/graphics** — not all images have text to extract

---

## Technology Stack

- OCR Engine: **Tesseract** (via `tesseract.js` for Node.js)
- Image Processing: **sharp** (resizing, format conversion, preprocessing)
- Text Parsing: Custom logic + regex patterns
- Storage: `ocr_results` table

### Supported File Types (OCR)

| File Type | OCR Processing | Notes |
|-----------|---------------|-------|
| **Images** (jpg, png, webp, gif, bmp, tiff) | Full OCR | Primary use case |
| **PDF** | Future: per-page OCR | Requires pdf-to-image conversion |
| **Text files** (.txt, .md, .csv) | Skip OCR, use raw text | No image processing needed |
| **Office docs** (.docx, .xlsx) | Skip OCR, extract text | Use document parsing libraries |
| **Binary/media** (video, audio, zip) | Skip entirely | Not processable |

**Note:** For non-image files that contain text directly (txt, docx, etc.), text extraction happens in the upload service, not the OCR pipeline. The OCR worker only processes image files.

---

## OCR Workflow

### 1. Pre-processing

Before OCR, images are optimized:

- Convert to grayscale (improves accuracy)
- Increase contrast
- Resize if too large (max 2000px width)
- Remove noise

### 2. Text Extraction

- Run Tesseract on preprocessed image
- Get raw text + confidence score
- Store full OCR output

### 3. Text Detection Analysis

Determine if meaningful text was found:

- Calculate **text density**: `characters / (width * height / 1000)`
- Check **confidence score** from Tesseract
- Analyze text quality (not just gibberish/noise)

**Classification thresholds:**

- `has_meaningful_text = true` if:
    - Text density > 5 chars per 1000px² AND
    - Confidence > 40% AND
    - Raw text length > 20 chars
- Otherwise `has_meaningful_text = false`

If no meaningful text detected:

- Skip metadata extraction
- Classify as `photo`, `screenshot`, or `graphic`
- Still store empty/minimal OCR result for consistency
- Skip LLM text processing (Plan 06), optionally use vision model instead

### 4. Metadata Extraction (if has_meaningful_text)

Parse structured data from text:

- **Dates**: Regex patterns for common formats (MM/DD/YYYY, DD.MM.YYYY, etc)
- **Companies**: Pattern matching + list of known entities
- **Values**: E.g. Currency amounts, stock quantities, percentages
- **Categories**: Document type classification

### 5. Storage

- Save to `ocr_results` table
- Update `documents.ocr_status = 'complete'`
- Update `documents.has_meaningful_text`
- Generate full-text search vector (only if has_meaningful_text)

---

## Folder Structure

```
backend/
  src/
    ocr/
      ocr.service.ts          // Main OCR orchestration
      tesseract.client.ts     // Tesseract wrapper
      image-preprocessor.ts   // Image optimization
      text-detector.ts        // Determine if meaningful text exists
      metadata-extractor.ts   // Parse structured data
      patterns.ts             // Regex patterns for extraction
      category-classifier.ts  // Document/image classification

    workers/
      ocr.worker.ts           // Job queue worker (from Plan 04)
```

---

## Database Schema Updates

Add to `documents` table (Plan 02 migration):

```sql
ALTER TABLE documents
ADD COLUMN has_meaningful_text BOOLEAN DEFAULT true;
```

Add to `ocr_results` table:

```sql
ALTER TABLE ocr_results
ADD COLUMN text_density FLOAT,
ADD COLUMN has_meaningful_text BOOLEAN DEFAULT true;
```

Update `DocumentCategory` enum to include new types:

```sql
-- If using enum type, extend it:
ALTER TYPE document_category ADD VALUE 'photo';
ALTER TYPE document_category ADD VALUE 'screenshot';
ALTER TYPE document_category ADD VALUE 'graphic';
```

---

## OCR Service Interface

```typescript
interface OcrService {
    processDocument(documentId: string): Promise<OcrResult>;
    extractMetadata(text: string): ExtractedMetadata;
    classifyDocument(text: string, metadata: ExtractedMetadata, hasMeaningfulText: boolean): DocumentCategory;
    detectTextPresence(ocrOutput: TesseractOutput, imageSize: ImageSize): TextDetectionResult;
}

interface OcrResult {
    raw_text: string;
    confidence_score: number;
    text_density: number; // chars per 1000 pixels²
    has_meaningful_text: boolean; // true if document contains real text
    metadata: ExtractedMetadata | null; // null if no meaningful text
    category: DocumentCategory;
}

interface TextDetectionResult {
    has_meaningful_text: boolean;
    text_density: number;
    confidence_score: number;
    raw_text_length: number;
    reason?: string; // e.g., "low_density", "low_confidence", "short_text"
}

interface ExtractedMetadata {
    dates: Date[];
    primary_date?: Date; // Best guess for "document date"
    companies: string[];
    currency_values: { amount: number; currency: string }[];
    percentages: number[];
}

type DocumentCategory =
    // Non-text content (photos, graphics)
    | 'photo' // Personal photos, images without text
    | 'screenshot' // Screen captures (may have some text but treated differently)
    | 'graphic' // Artwork, diagrams, illustrations

    // Common document types
    | 'receipt' // Purchase receipts, invoices
    | 'invoice' // Bills, invoices
    | 'statement' // Bank statements, account statements
    | 'letter' // Correspondence, emails
    | 'contract' // Legal agreements, contracts
    | 'form' // Filled forms, applications
    | 'certificate' // Certificates, licenses
    | 'report' // Reports, analyses
    | 'article' // News articles, blog posts
    | 'memo' // Internal memos, notes
    | 'newsletter' // Newsletters, publications

    // Financial documents (common use case)
    | 'stock_statement' // Stock/investment statements
    | 'dividend_notice' // Dividend notifications
    | 'tax_document' // Tax forms, returns
    | 'other'; // Uncategorized documents with text
```

---

## Metadata Extraction Rules

### Date Detection

Priority order:

1. Top-right corner text (common for financial docs)
2. Header lines
3. Any date in format MM/DD/YYYY or DD.MM.YYYY
4. Fallback: use document upload date

### Company Extraction

- Match against known stock tickers (AAPL, GOOGL, etc)
- Pattern: "Company Name Inc.", "X Corporation"
- Store all mentions, rank by frequency

### Value Extraction

- Currency: `$1,234.56`, `USD 1234.56`, `€1.234,56`
- Stock quantities: Numbers near keywords "shares", "units", "qty"
- Percentages: `12.5%`, `0.05%`

### Category Classification

**Step 1: Check if text was detected**

If `has_meaningful_text = false`, classify based on image characteristics:

- Default to `photo`
- If aspect ratio matches common screen sizes (16:9, 16:10) → `screenshot`
- If image has very uniform colors or vector-like edges → `graphic`

**Step 2: Keyword-based classification (if has text)**

Simple keyword-based rules with scoring (multiple matches increase confidence):

| Keywords                                       | Category          |
| ---------------------------------------------- | ----------------- |
| "receipt", "purchase", "total", "subtotal"     | `receipt`         |
| "invoice", "bill to", "due date", "amount due" | `invoice`         |
| "statement", "account", "balance", "period"    | `statement`       |
| "dear", "sincerely", "regards", letter format  | `letter`          |
| "agreement", "contract", "parties", "hereby"   | `contract`        |
| "form", "please fill", "signature", checkboxes | `form`            |
| "certificate", "certify", "awarded", "license" | `certificate`     |
| "report", "analysis", "findings", "conclusion" | `report`          |
| "memo", "memorandum", "to:", "from:", "re:"    | `memo`            |
| "newsletter", "subscribe", "issue", "edition"  | `newsletter`      |
| "dividend", "shares", "stock", "portfolio"     | `stock_statement` |
| "tax", "W-2", "1099", "return", "IRS"          | `tax_document`    |

- If no strong keyword matches → `other`
- Note: LLM (Plan 06) will refine this classification with better accuracy

---

## Error Handling

### No Meaningful Text Detected

If OCR determines `has_meaningful_text = false`:

- **This is NOT an error** — it's a valid result for photos/graphics
- Store minimal OCR result (empty text, low confidence)
- Classify as `photo`, `screenshot`, or `graphic`
- Set `documents.has_meaningful_text = false`
- Skip LLM text summarization (Plan 06)
- Optionally queue `llm_vision` job to describe image content (if vision model enabled)
- Still generate thumbnails normally
- Do NOT index for full-text search (nothing to index)

### Low Confidence OCR (with text)

If Tesseract confidence < 60% but `has_meaningful_text = true`:

- Store result but flag `needs_review = true` in UI
- Suggest manual review
- Allow user to re-process with different settings
- Still attempt metadata extraction (may yield partial results)

### Very Low Confidence with Text

If confidence < 30% but text was detected:

- Likely a borderline case (handwriting, poor scan quality)
- Store raw text but mark as unreliable
- Skip LLM processing (garbage in = garbage out)
- Suggest user re-scan or manually categorize

### Extraction Failures

If no metadata extracted (but text exists):

- Still store raw text (useful for search)
- Category defaults to `other`
- Don't block document save
- LLM may still extract useful metadata (Plan 06)

### Invalid Images

- Corrupted files
- Non-image files
- Encrypted/password-protected PDFs (future)
- Zero-byte files

Mark job as `failed`, log error, notify user.

---

## Performance Considerations

### Batch Processing

For bulk uploads (100+ images):

- Process in parallel (configurable concurrency)
- Use job priority: user-triggered > batch
- Progress tracking via job status

### Caching

- Cache preprocessed images (optional)
- Cache Tesseract model loads (loaded once per worker)

### Resource Limits

- Max image size: 10MB (reject larger)
- Max processing time: 30s per image
- Timeout and retry if exceeded

---

## Integration with Job Queue

OCR Worker flow:

```typescript
async function processOcrJob(job: Job<OcrJobData>) {
    const { documentId } = job.data;

    // 1. Fetch document from DB
    const doc = await db.selectFrom('documents').where('id', '=', documentId).selectAll().executeTakeFirst();

    // 2. Load image from storage
    const imageBuffer = await storageService.retrieve(doc.file_path);

    // 3. Get image dimensions
    const imageSize = await sharp(imageBuffer).metadata();

    // 4. Preprocess
    const processedImage = await preprocessImage(imageBuffer);

    // 5. OCR
    const ocrOutput = await tesseractClient.recognize(processedImage);

    // 6. Detect if meaningful text exists
    const textDetection = detectTextPresence(ocrOutput, {
        width: imageSize.width!,
        height: imageSize.height!,
    });

    let metadata: ExtractedMetadata | null = null;
    let category: DocumentCategory;

    if (textDetection.has_meaningful_text) {
        // 7a. Extract metadata (only if text found)
        metadata = await metadataExtractor.extract(ocrOutput.text);

        // 7b. Classify document
        category = classifyDocument(ocrOutput.text, metadata, true);
    } else {
        // 7c. Classify as non-text content
        category = classifyNonTextImage(imageSize, doc.original_filename);
    }

    // 8. Save to DB
    await saveOcrResult(documentId, {
        raw_text: ocrOutput.text,
        confidence_score: ocrOutput.confidence,
        text_density: textDetection.text_density,
        has_meaningful_text: textDetection.has_meaningful_text,
        metadata,
    });

    await db
        .updateTable('documents')
        .set({
            document_category: category,
            has_meaningful_text: textDetection.has_meaningful_text,
            ocr_status: 'complete',
        })
        .where('id', '=', documentId)
        .execute();

    // 9. Conditionally queue LLM job
    if (textDetection.has_meaningful_text && ocrOutput.confidence > 30) {
        await llmQueue.add('process', { documentId, type: 'text_summary' });
    } else if (!textDetection.has_meaningful_text && env.LLM_VISION_ENABLED) {
        // Optionally describe photos with vision model
        await llmQueue.add('process', { documentId, type: 'vision_describe' });
    }

    // 10. Update full-text search index (only if text exists)
    if (textDetection.has_meaningful_text) {
        await updateSearchIndex(documentId, ocrOutput.text);
    }

    return {
        success: true,
        confidence: ocrOutput.confidence,
        has_meaningful_text: textDetection.has_meaningful_text,
        category,
    };
}

function classifyNonTextImage(imageSize: ImageSize, filename: string): DocumentCategory {
    const aspectRatio = imageSize.width / imageSize.height;

    // Common screenshot aspect ratios
    const isScreenshotRatio =
        Math.abs(aspectRatio - 16 / 9) < 0.1 || // 16:9
        Math.abs(aspectRatio - 16 / 10) < 0.1 || // 16:10
        Math.abs(aspectRatio - 4 / 3) < 0.1; // 4:3

    // Check filename hints
    const lowerFilename = filename.toLowerCase();
    if (lowerFilename.includes('screenshot') || lowerFilename.includes('screen')) {
        return 'screenshot';
    }

    if (isScreenshotRatio && imageSize.width >= 1024) {
        return 'screenshot';
    }

    // Default to photo for images without text
    return 'photo';
}
```

---

## Search Integration

After OCR completion, update full-text search (only for documents with text):

```sql
-- Only update search vector if meaningful text was found
UPDATE ocr_results
SET text_vector = CASE
    WHEN has_meaningful_text THEN to_tsvector('english', raw_text)
    ELSE NULL
END
WHERE document_id = :documentId;
```

For photos/graphics without text:

- Not indexed for full-text search
- Can still be found via: filename, folder, category, date, tags
- If LLM vision is used, the generated description can be indexed

---

## API Endpoints (Exposed by routes)

### Trigger OCR

```
POST /api/documents/:id/ocr
Response: { job_id: string, status: 'pending' }
```

### Get OCR Result

```
GET /api/documents/:id/ocr
Response: { raw_text, confidence_score, metadata, category }
```

### Re-process OCR

```
POST /api/documents/:id/ocr/retry
```

---

## Acceptance Criteria

- Tesseract is installed and accessible
- OCR worker processes jobs successfully
- Text extraction works on sample document images
- **Photos without text are correctly detected and classified as `photo`**
- **`has_meaningful_text` flag is accurately set**
- Metadata extraction finds dates and companies (for documents with text)
- Results stored in `ocr_results` table
- Full-text search vector is generated (only for documents with text)
- Document category is automatically assigned
- Failed OCR attempts are logged and retried
- **LLM job is only queued for documents with meaningful text**

---

## Future Enhancements

- PDF support (multi-page documents)
- Handwriting recognition (Tesseract LSTM models)
- Custom model training for specific document types
- OCR quality scoring and auto-retry
- User feedback loop (correct wrong extractions)
- **LLM vision model integration** for describing photos/graphics
- **Pre-OCR triage**: Use lightweight heuristics (edge detection, color variance) to skip OCR entirely for obvious photos
- **Image content detection**: Detect faces, objects, scenes for better photo categorization
- **Smart screenshot detection**: Parse UI elements, detect app names from screenshots
